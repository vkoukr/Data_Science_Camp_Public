{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38f9c7c2-d966-433d-b918-f3ad2383c53e",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "# Home Work\n",
    " Natural Language Toolkit (NLTK)\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de07d752-0c55-4c1a-88fd-24f9c1409f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import gutenberg "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e4b4c69-f5d4-4d5c-99d2-68612e3a3041",
   "metadata": {},
   "outputs": [],
   "source": [
    "moby_raw = gutenberg.raw('melville-moby_dick.txt') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0cc7d0-7c5c-4066-a473-0aaaf2a68cb8",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "### Example 1\n",
    "\n",
    "</font>\n",
    "\n",
    "How many tokens (words and punctuation symbols) are in `moby_raw`?\n",
    "<br>*This function should return an integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c92c8334-e5bd-43e4-91b4-0a42a5d36848",
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_one():\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    return len(word_tokenize(moby_raw)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30125c05-0aa8-42c4-afd6-f1e0f616c4fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "255,028\n"
     ]
    }
   ],
   "source": [
    "print ('{:,}'.format(example_one()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201f5b89-5c33-49bd-b36a-e8808cf24560",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "### Example 2\n",
    "\n",
    "</font>\n",
    "\n",
    "How many unique tokens (unique words and punctuation) does `moby_raw` have?\n",
    "<br>*This function should return an integer.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0df483fe-1429-4343-ada3-e87dc11cb306",
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_two():    \n",
    "    return len(set(nltk.word_tokenize(moby_raw)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbd83bf-9dc2-44ac-8bbb-572b8cddfc71",
   "metadata": {},
   "source": [
    "<font color = blue >\n",
    "\n",
    "### Check result\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "913fbabd-298f-4d76-ae38-bc22afaeb565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20,742\n"
     ]
    }
   ],
   "source": [
    "print ('{:,}'.format(example_two()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1b4b3d-5005-4a76-8727-3a942f6536d9",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "### Example 3\n",
    "\n",
    "</font>\n",
    "\n",
    "After lemmatizing the verbs, how many unique tokens does `moby_raw` have?\n",
    "<br>*This function should return an integer.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3c060a4-2f02-426a-bdec-4901844f5f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def example_three():\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized = [lemmatizer.lemmatize(w,'v') for w in nltk.word_tokenize(moby_raw)]\n",
    "    return len(set(lemmatized))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc8e0b1-d10a-4d0d-a853-7cab165789ac",
   "metadata": {},
   "source": [
    "<font color = blue >\n",
    "\n",
    "### Check result\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0403a3c-9bce-4bde-8e90-5a1e85ca73bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16,887\n"
     ]
    }
   ],
   "source": [
    "print ('{:,}'.format(example_three()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9dc6d0e-bea0-4301-9cf9-71ce1f84e5fd",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "### Question 1\n",
    "\n",
    "</font>\n",
    "\n",
    "\n",
    "What is the lexical diversity of the given text input? (i.e. ratio of unique tokens to the total number of tokens)\n",
    "<br>*This function should return a float.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "58679cce-388c-4f33-9caf-c1f87997a6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_one():\n",
    "    return example_two()/example_one()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa7081c-773c-4054-9560-844fa2d6fa01",
   "metadata": {},
   "source": [
    "<font color = blue >\n",
    "\n",
    "### Check result\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d949e3a-b4e5-4694-b093-6bd502cadf43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08133224587104161"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_one()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1c0cc1-d925-4bb7-88e9-4714e751f111",
   "metadata": {},
   "source": [
    "<font color = blue >\n",
    "\n",
    "### Expected Output\n",
    "\n",
    "</font>\n",
    "\n",
    "`0.08139566804842562`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b5a80b-0eb9-4f56-9178-c4c777cdf9c7",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "### Question 2\n",
    "\n",
    "</font>\n",
    "\n",
    "What percentage of tokens is 'whale'or 'Whale'?\n",
    "<br>*This function should return a float.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d66a4a44-492a-4f97-965a-dc10bcceff2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def answer_two():    \n",
    "    from nltk.tokenize import word_tokenize\n",
    "    all_tokens=word_tokenize(moby_raw)\n",
    "    indices = [i for i, x in enumerate(all_tokens) if ((x == \"whale\") | (x=='Whale'))]   # -- Number of tokens 'whale' or \"Whale\"\n",
    "    return len(indices)/len(all_tokens)*100\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fa874d-fb51-4710-bd25-6a5e873edfa4",
   "metadata": {},
   "source": [
    "<font color = blue >\n",
    "\n",
    "### Check result\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "46d562ec-c7ae-445f-866b-f4f4d5aedefe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4125037250811676"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_two()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c95453a-96de-4cd5-85bc-e0eb854ebf0f",
   "metadata": {},
   "source": [
    "<font color = blue >\n",
    "\n",
    "### Expected Output\n",
    "\n",
    "</font>\n",
    "\n",
    "`0.4125668166077752`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f935a47b-f47d-4784-9cb2-65f0ed78ecd6",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "### Question 3\n",
    "\n",
    "</font>\n",
    "\n",
    "What are the 20 most frequently occurring (unique) tokens in the text? What is their frequency?\n",
    "<br>*This function should return a list of 10 tuples where each tuple is of the form `(token, frequency)`. The list should be sorted in descending order of frequency.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c80368e3-b7c4-4fa2-b686-807a7a405492",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_three():\n",
    "    from nltk import FreqDist\n",
    "    all_tokens=nltk.word_tokenize(moby_raw)\n",
    "    dist = FreqDist(all_tokens) # the same as text1.vocab() \n",
    "    #print(dist)\n",
    "    #dist\n",
    "    return [(x,dist[x]) for x in list(dist)[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0db2d566-4f4e-4cce-bd85-3f444906eb2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 19204),\n",
       " ('the', 13715),\n",
       " ('.', 7306),\n",
       " ('of', 6513),\n",
       " ('and', 6010),\n",
       " ('a', 4545),\n",
       " ('to', 4515),\n",
       " (';', 4173),\n",
       " ('in', 3908),\n",
       " ('that', 2978)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import FreqDist\n",
    "all_tokens=nltk.word_tokenize(moby_raw)\n",
    "dist = FreqDist(all_tokens)\n",
    "dist.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e83508-ae26-4184-a384-f6397a2d11e6",
   "metadata": {},
   "source": [
    "<font color = blue >\n",
    "\n",
    "### Check result\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "24b08bbc-3121-43d7-87f6-4e2e9c0eed55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(',', 19204), ('the', 13715), ('.', 7306), ('of', 6513), ('and', 6010), ('a', 4545), ('to', 4515), (';', 4173), ('in', 3908), ('that', 2978)]\n"
     ]
    }
   ],
   "source": [
    "print(answer_three())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de851826-3376-4db3-8d81-b15d3f794467",
   "metadata": {},
   "source": [
    "<font color = blue >\n",
    "\n",
    "### Expected Output\n",
    "\n",
    "</font>\n",
    "\n",
    "`[(',', 19204),\n",
    " ('the', 13715),\n",
    " ('.', 7308),\n",
    " ('of', 6513),\n",
    " ('and', 6010),\n",
    " ('a', 4545),\n",
    " ('to', 4515),\n",
    " (';', 4173),\n",
    " ('in', 3908),\n",
    " ('that', 2978)]`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057e7fea-b8ba-47a2-b873-8264738ace54",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "### Question 4\n",
    "\n",
    "</font>\n",
    "\n",
    "What tokens have a length of greater than 5 and frequency of more than 150?\n",
    "<br>*This function should return a sorted list of the tokens that match the above constraints. To sort your list, use `sorted()`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f000da2c-b5eb-410b-ae0a-de8c56683d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_four():\n",
    "    from nltk import FreqDist\n",
    "    all_tokens=nltk.word_tokenize(moby_raw)\n",
    "    dist = FreqDist(all_tokens) # the same as text1.vocab() \n",
    "    vocab=dist.keys()\n",
    "    freq_tokens = [w for w in vocab if len(w) > 5 and dist[w] > 150] \n",
    "    return sorted(freq_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369a72d9-fed7-4669-bc89-52672e11ca2e",
   "metadata": {},
   "source": [
    "<font color = blue >\n",
    "\n",
    "### Check result\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4d2f209c-52eb-4b8d-ba16-fd2fe109a89f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Captain', 'Pequod', 'Queequeg', 'Starbuck', 'almost', 'before', 'himself', 'little', 'seemed', 'should', 'though', 'through', 'whales', 'without']\n"
     ]
    }
   ],
   "source": [
    "print(answer_four())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960b3abd-7ff8-4c8b-9798-d1237bc8d167",
   "metadata": {},
   "source": [
    "<font color = blue >\n",
    "\n",
    "### Expected Output\n",
    "\n",
    "</font>\n",
    "\n",
    "`['Captain', 'Pequod', 'Queequeg', 'Starbuck', 'almost', 'before', 'himself', 'little', 'seemed', 'should', 'though', 'through', 'whales', 'without']`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8eba23-1ae7-447d-995b-51df0ea18be0",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "### Question 5\n",
    "\n",
    "</font>\n",
    "\n",
    "Find the longest word in text1 and that word's length.\n",
    "<br>\n",
    "*This function should return a tuple `(longest_word, length)`.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6fa0e2ab-53bc-4336-9541-83a5ed214fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_five():\n",
    "    from nltk import FreqDist\n",
    "    all_tokens=nltk.word_tokenize(moby_raw)\n",
    "    dist = FreqDist(all_tokens) # the same as text1.vocab() \n",
    "    vocab=dist.keys()\n",
    "    t=[[x,len(x)] for x in vocab]\n",
    "    index = max(t, key=lambda item: item[1])\n",
    "    return (index[0], index[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8550f9c7-a564-4e93-acd0-0b027c5b9d4f",
   "metadata": {},
   "source": [
    "<font color = blue >\n",
    "\n",
    "### Check result\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b613f960-51eb-4778-9410-bd42c8e70735",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"twelve-o'clock-at-night\", 23)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_five()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6264b602-bf4d-4320-8d20-db27360969a8",
   "metadata": {},
   "source": [
    "<font color = blue >\n",
    "\n",
    "### Expected Output\n",
    "\n",
    "</font>\n",
    "\n",
    "`(\"twelve-o'clock-at-night\", 23)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13e681e-e651-466c-b39b-838b58a09433",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "### Question 6\n",
    "\n",
    "</font>\n",
    "\n",
    "What unique words have a frequency of more than 2000? What is their frequency?\n",
    "<br>*This function should return a list of tuples of the form `(frequency, word)` sorted in descending order of frequency.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c830b80b-df9a-4ad8-b1da-315039f42757",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_six():\n",
    "    from nltk import FreqDist\n",
    "    import pandas as pd\n",
    "    \n",
    "    all_tokens=nltk.word_tokenize(moby_raw)\n",
    "    dist = FreqDist(all_tokens)\n",
    "    vocab=dist.keys()\n",
    "    uniq_tokens=set(nltk.word_tokenize(moby_raw))\n",
    "    freq_tokens = [(w, dist[w]) for w in uniq_tokens if ((dist[w] > 2000) & (w.isalpha()))] \n",
    "    df=pd.DataFrame(freq_tokens,columns=[\"token\", \"frequency\"])\n",
    "    df=df.sort_values(by='frequency', ascending=False)\n",
    "    return list(zip(df.frequency, df.token))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf18287-f8e2-42f3-a083-14ba6e60a69b",
   "metadata": {},
   "source": [
    "<font color = blue >\n",
    "\n",
    "### Check result\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "15b7395e-2ea8-4dcc-baff-c31d42715f3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(13715, 'the'), (6513, 'of'), (6010, 'and'), (4545, 'a'), (4515, 'to'), (3908, 'in'), (2978, 'that'), (2459, 'his'), (2196, 'it'), (2113, 'I')]\n"
     ]
    }
   ],
   "source": [
    "print(answer_six())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8921fa2-a5f2-41c0-a3f6-9c8533f9672c",
   "metadata": {},
   "source": [
    "<font color = blue >\n",
    "\n",
    "### Expected Output\n",
    "\n",
    "</font>\n",
    "\n",
    "`[(13715, 'the'), (6513, 'of'), (6010, 'and'), (4545, 'a'), (4515, 'to'), (3908, 'in'), (2978, 'that'), (2459, 'his'), (2196, 'it'), (2097, 'I')]`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381cf7ea-5e38-47ff-9ad2-f6331c9c4b17",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "### Question 7\n",
    "\n",
    "</font>\n",
    "\n",
    "What is the average number of tokens per sentence?\n",
    "<br>*This function should return a float.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "22e4875e-6ca1-4b25-9746-ec7445796b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "356ed805-9d73-4c49-856d-9ed0eb4d4e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_seven():\n",
    "    sentences = sent_tokenize(moby_raw)\n",
    "    counts = (len(nltk.word_tokenize(sentence)) for sentence in sentences)\n",
    "    return sum(counts)/float(len(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4453e5af-676f-440a-b5d0-869a969f62e5",
   "metadata": {},
   "source": [
    "<font color = blue >\n",
    "\n",
    "### Check result\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8cd56507-03b5-4d55-b1c5-d4c868459502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.88591149005278\n"
     ]
    }
   ],
   "source": [
    "print(answer_seven())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef7db6d-5bb8-44c2-947a-074947774489",
   "metadata": {},
   "source": [
    "<font color = blue >\n",
    "\n",
    "### Expected Output\n",
    "\n",
    "</font>\n",
    "\n",
    "`25.881952902963864`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb34bad-d523-4a9a-a0e1-68f7f2eedd78",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "### Question 8\n",
    "\n",
    "</font>\n",
    "\n",
    "What are the 5 most frequent parts of speech in this text? What is their frequency?\n",
    "<br>*This function should return a list of tuples of the form `(part_of_speech, frequency)` sorted in descending order of frequency.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5ba34908-12ef-4b4c-ba5a-40ef3caa1361",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_eight():\n",
    "    from nltk import FreqDist\n",
    "    import pandas as pd\n",
    "    \n",
    "    all_tokens=nltk.word_tokenize(moby_raw)\n",
    "    dist = FreqDist(all_tokens)\n",
    "    df=pd.DataFrame(dist.most_common(), columns=[\"token\", \"frequency\"])\n",
    "    tagged = nltk.pos_tag(df.token)\n",
    "    frequencies = FreqDist([tag for (word, tag) in tagged])\n",
    "    return frequencies.most_common(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d74c654-256d-4315-af64-1ba35964a391",
   "metadata": {},
   "source": [
    "<font color = blue >\n",
    "\n",
    "### Check result\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7b1602c5-7a7f-4c66-a89d-b7bc2d061fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('JJ', 4268), ('NN', 4208), ('NNP', 3153), ('NNS', 2654), ('VBD', 1392)]\n"
     ]
    }
   ],
   "source": [
    "print(answer_eight())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0741de-8160-4029-bdfd-80386946882f",
   "metadata": {},
   "source": [
    "<font color = blue >\n",
    "\n",
    "### Expected Output\n",
    "\n",
    "</font>\n",
    "\n",
    "`[('NN', 32730), ('IN', 28657), ('DT', 25867), (',', 19204), ('JJ', 17620)]`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebfbd40-e349-4906-ab8b-9bb36cf49e45",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "### Question 9\n",
    "\n",
    "</font>\n",
    "\n",
    "Create spelling recommender, that take a list of misspelled words and recommends a correctly spelled word for every word in the list.\n",
    "\n",
    "For every misspelled word, the recommender should find find the word in `correct_spellings` that has the shortest `edit distance` (you may need  to use `nltk.edit_distance(word_1, word_2, transpositions=True)`), and starts with the same letter as the misspelled word, and return that word as a recommendation.\n",
    "\n",
    "Recommender should provide recommendations for the three words: `['cormulent', 'incendenece', 'validrate']`.\n",
    "<br>*This function should return a list of length three:\n",
    "`['cormulent_reccomendation', 'incendenece_reccomendation', 'validrate_reccomendation']`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "671d1c56-888b-4a22-abbc-1504a01762d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c6ce52f-380c-4caa-9cad-4ce9619a41c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_nine(default_words= ['cormulent', 'incendenece', 'validrate']):    \n",
    "    import pandas as pd\n",
    "    from nltk.corpus import words\n",
    "    from nltk.metrics.distance import (\n",
    "        edit_distance,\n",
    "        jaccard_distance,\n",
    "        )\n",
    "    from nltk.util import ngrams\n",
    "    correct_spellings = words.words()\n",
    "    spellings_series = pd.Series(correct_spellings)\n",
    "    outcomes = []\n",
    "    gram_number=3\n",
    "    for word_from_default in default_words:\n",
    "        spellings = spellings_series[spellings_series.str.startswith(word_from_default[0])]\n",
    "        distances = ((jaccard_distance(set(ngrams(word_from_default, gram_number)),\n",
    "                                       set(ngrams(word, gram_number))), word)\n",
    "                     for word in spellings)\n",
    "        closest = min(distances)\n",
    "        outcomes.append(closest[1])\n",
    "    return outcomes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681d1d39-9352-4987-8f88-a1e0c3220013",
   "metadata": {},
   "source": [
    "<font color = blue >\n",
    "\n",
    "### Check result\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51b57b8d-d58f-440a-9c68-bf9516f68f9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['corpulent', 'indecence', 'validate']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_nine()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902add40-b364-4521-b155-099a5d95f072",
   "metadata": {},
   "source": [
    "<font color = blue >\n",
    "\n",
    "### Expected Output\n",
    "\n",
    "</font>\n",
    "\n",
    "`['corpulent', 'intendence', 'validate']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6715d5-9f7d-488c-84e8-7235359b89cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
