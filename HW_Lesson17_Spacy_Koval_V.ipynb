{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW_Lesson17_Spacy_Koval_V.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPVhhIUlG9d9+fPHvjET5m2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vkoukr/Data_Science_Camp_Public/blob/main/HW_Lesson17_Spacy_Koval_V.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o16KWtUjxTv7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f58faff9-3d5d-49f0-d98d-198286ab763c"
      },
      "source": [
        "# import spacy\n",
        "# nlp = spacy.load(\"en_core_web_lg\")\n",
        "\n",
        "\n",
        "import spacy.cli\n",
        "spacy.cli.download(\"en_core_web_lg\")\n",
        "import en_core_web_lg\n",
        "nlp = en_core_web_lg.load()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_lg')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bqf5COfz8LZO"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NyMG5SHcLh6s",
        "outputId": "6a155a39-afe3-4a2f-b159-2461e5b1f9a5"
      },
      "source": [
        "nlp.pipe_names"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['tagger', 'parser', 'ner']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Y5Sh1pGTYh9"
      },
      "source": [
        "# **Defining Random English Text**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4n2AOxxBLlmR"
      },
      "source": [
        "# text = \"\"\"He determined to drop his litigation with the monastry, and relinguish his claims to the wood-cuting and \n",
        "# fishery rihgts at once. He was the more ready to do this becuase the rights had become much less valuable, and he had \n",
        "# indeed the vaguest idea where the wood and river in question were.\"\"\"\n",
        "\n",
        "text = \"\"\"How do you explain this progression?\n",
        "Cigarettes are linked to 85% of lung cancer cases, this massively damages people's health.\n",
        "Everything moves very fast in football\n",
        "You're never going to win 4-0 every weekend - we're not FC Barcelona!\n",
        "We got out of Afghanistan.\n",
        "French troops have left their area of responsibility in Afghanistan \"\"\"\n",
        "\n",
        "my_doc = nlp(text)"
      ],
      "execution_count": 167,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ZnpSCR6T1SG"
      },
      "source": [
        "# **Tokenizing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sC9mrkY5P4w7",
        "outputId": "ebf3936e-40ef-4dcb-b99f-f11183b61df8"
      },
      "source": [
        "#--Tpkenizing variant 1---\n",
        "token_list = []\n",
        "for token in my_doc:\n",
        "      token_list.append(token.text)\n",
        "print('-----Tokens form 1 variant-----')\n",
        "print(token_list)\n",
        "\n",
        "#--Tpkenizing variant 2---\n",
        "token_list_tokenized = []\n",
        "tokenized = nlp.tokenizer(text)\n",
        "for token in tokenized:\n",
        "      token_list_tokenized.append(token.text)\n",
        "print('\\n-----Tokens form 2 variant-----')\n",
        "print(token_list_tokenized)"
      ],
      "execution_count": 182,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----Tokens form 1 variant-----\n",
            "['How', 'do', 'you', 'explain', 'this', 'progression', '?', '\\n', 'Cigarettes', 'are', 'linked', 'to', '85', '%', 'of', 'lung', 'cancer', 'cases', ',', 'massively', 'damages', 'people', \"'s\", 'health', '.', 'Everything', 'moves', 'very', 'fast', 'in', 'football', 'You', \"'re\", 'never', 'going', 'win', '4', '-', '0', 'every', 'weekend', 'we', 'not', 'FC', 'Barcelona', '!', 'We', 'got', 'out', 'Afghanistan', 'French', 'troops', 'have', 'left', 'their', 'area', 'responsibility']\n",
            "\n",
            "-----Tokens form 2 variant-----\n",
            "['How', 'do', 'you', 'explain', 'this', 'progression', '?', '\\n', 'Cigarettes', 'are', 'linked', 'to', '85', '%', 'of', 'lung', 'cancer', 'cases', ',', 'this', 'massively', 'damages', 'people', \"'s\", 'health', '.', '\\n', 'Everything', 'moves', 'very', 'fast', 'in', 'football', '\\n', 'You', \"'re\", 'never', 'going', 'to', 'win', '4', '-', '0', 'every', 'weekend', '-', 'we', \"'re\", 'not', 'FC', 'Barcelona', '!', '\\n', 'We', 'got', 'out', 'of', 'Afghanistan', '.', '\\n', 'French', 'troops', 'have', 'left', 'their', 'area', 'of', 'responsibility', 'in', 'Afghanistan', 'area']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tLcYJ67aM_c"
      },
      "source": [
        "# **Removing StopWords from the text**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJ1BBy-XUH3r",
        "outputId": "0f4e0e22-594b-412c-f45e-2d759f103ffd"
      },
      "source": [
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "\n",
        "# Create list of word tokens after removing stopwords\n",
        "filtered_sentence =[] \n",
        "for word in token_list_tokenized:\n",
        "    lexeme = nlp.vocab[word]\n",
        "    if (lexeme.is_stop == False) and (lexeme.is_punct== False) and (lexeme not in filtered_sentence):\n",
        "        filtered_sentence.append(word) \n",
        "print('---- Original tokens-----')        \n",
        "print(token_list_tokenized)\n",
        "print('\\n---- Filtered from StopWords tokens-----')\n",
        "print(filtered_sentence)   \n"
      ],
      "execution_count": 183,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---- Original tokens-----\n",
            "['How', 'do', 'you', 'explain', 'this', 'progression', '?', '\\n', 'Cigarettes', 'are', 'linked', 'to', '85', '%', 'of', 'lung', 'cancer', 'cases', ',', 'this', 'massively', 'damages', 'people', \"'s\", 'health', '.', '\\n', 'Everything', 'moves', 'very', 'fast', 'in', 'football', '\\n', 'You', \"'re\", 'never', 'going', 'to', 'win', '4', '-', '0', 'every', 'weekend', '-', 'we', \"'re\", 'not', 'FC', 'Barcelona', '!', '\\n', 'We', 'got', 'out', 'of', 'Afghanistan', '.', '\\n', 'French', 'troops', 'have', 'left', 'their', 'area', 'of', 'responsibility', 'in', 'Afghanistan', 'area']\n",
            "\n",
            "---- Filtered from StopWords tokens-----\n",
            "['explain', 'progression', '\\n', 'Cigarettes', 'linked', '85', 'lung', 'cancer', 'cases', 'massively', 'damages', 'people', 'health', 'moves', 'fast', 'football', 'going', 'win', '4', '0', 'weekend', 'FC', 'Barcelona', 'got', 'Afghanistan', 'French', 'troops', 'left', 'area', 'responsibility']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hDulfswyr7bc"
      },
      "source": [
        "# **Extracting NOUNS, VERBS, NUMBERS from filtered text**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WhieeM0RP-Py"
      },
      "source": [
        "full_data = nlp(' '.join(filtered_sentence))\n",
        "# print(full_data,'\\n')\n",
        "index = 0\n",
        "nounIndices = []\n",
        "verbIndices = []\n",
        "numIndices = []\n",
        "for token in full_data:\n",
        "    # print('Word is:', token.text, token.pos_, token.dep_, token.head.text)\n",
        "    # print('Word is:', token.text, token.pos_)\n",
        "    # if token.pos_ == 'NOUN':\n",
        "    if token.pos_ == 'NOUN' or token.pos_ == 'PROPN':\n",
        "        nounIndices.append(index)\n",
        "    if token.pos_ == 'VERB':\n",
        "        verbIndices.append(index)\n",
        "    if token.pos_ == 'NUM':\n",
        "        numIndices.append(index)\n",
        "    index = index + 1\n",
        "\n",
        "# print('---NOUNs has a positions: ',nounIndices)\n",
        "# print('List of NOUNs:')\n",
        "# for i in nounIndices: print('\\t',full_data[i])\n",
        "# print('---VERBs has a positions: ',nounIndices)\n",
        "# print('List of VERBs:')\n",
        "# for i in verbIndices: print('\\t',full_data[i])\n",
        "# print('---NUMBERs has a positions: ',nounIndices)\n",
        "# print('List of NUMBERs:')\n",
        "# for i in numIndices: print('\\t',full_data[i])\n"
      ],
      "execution_count": 184,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-M5fLqvt7Hfu"
      },
      "source": [
        "# **Collecting the freequencies for NOUNE,VERB,NUMB and printing sorted results**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vq8HOyyml64o",
        "outputId": "b48d4c42-2cde-43f2-bf72-3707568a343a"
      },
      "source": [
        "from collections import Counter     # --- For frequency collection\n",
        "word_freq = Counter(token_list_tokenized)\n",
        "\n",
        "df_noune = pd.DataFrame(columns=['Noune', 'Freq'])\n",
        "for i in nounIndices:\n",
        "  # print(, full_data[i])\n",
        "  df_noune = df_noune.append({'Noune': filtered_sentence[i], 'Freq': word_freq.get(filtered_sentence[i])}, ignore_index=True)\n",
        "df_noune=df_noune.sort_values(by=['Freq'], ascending=False)\n",
        "print('\\n',df_noune)\n",
        "\n",
        "df_verb = pd.DataFrame(columns=['Verb', 'Freq'])\n",
        "for i in verbIndices:\n",
        "  # print(, full_data[i])\n",
        "  df_verb = df_verb.append({'Verb': filtered_sentence[i], 'Freq': word_freq.get(filtered_sentence[i])}, ignore_index=True)\n",
        "df_verb=df_verb.sort_values(by=['Freq'], ascending=False)\n",
        "print('\\n',df_verb)\n",
        "\n",
        "df_num = pd.DataFrame(columns=['Numb', 'Freq'])\n",
        "for i in numIndices:\n",
        "  # print(, full_data[i])\n",
        "  df_num = df_num.append({'Numb': filtered_sentence[i], 'Freq': word_freq.get(filtered_sentence[i])}, ignore_index=True)\n",
        "df_num=df_num.sort_values(by=['Freq'], ascending=False)\n",
        "print('\\n',df_num)"
      ],
      "execution_count": 190,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "              Noune Freq\n",
            "15            area    2\n",
            "13     Afghanistan    2\n",
            "0      progression    1\n",
            "9              win    1\n",
            "14          troops    1\n",
            "12       Barcelona    1\n",
            "11              FC    1\n",
            "10         weekend    1\n",
            "8         football    1\n",
            "1       Cigarettes    1\n",
            "7            moves    1\n",
            "6           health    1\n",
            "5           people    1\n",
            "4            cases    1\n",
            "3           cancer    1\n",
            "2             lung    1\n",
            "16  responsibility    1\n",
            "\n",
            "       Verb Freq\n",
            "0  explain    1\n",
            "1   linked    1\n",
            "2  damages    1\n",
            "3    going    1\n",
            "4      got    1\n",
            "5     left    1\n",
            "\n",
            "   Numb Freq\n",
            "0   85    1\n",
            "1    4    1\n",
            "2    0    1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SnxqofhvPm1u"
      },
      "source": [
        "# **Extracting Named Entities {'LOCATION', 'PERSON', 'ORGANIZATION', 'MISC.'}**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yoyhkZ-7yyA-",
        "outputId": "b88e8010-7ff4-4b2b-ec56-df3aac327372"
      },
      "source": [
        "labels = set([w.label_ for w in my_doc.ents]) \n",
        "# print(labels)\n",
        "for label in labels: \n",
        "    entities = [e.string for e in my_doc.ents if label==e.label_] \n",
        "    entities = list(set(entities)) \n",
        "    print( label,entities)"
      ],
      "execution_count": 224,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PERCENT ['85% ']\n",
            "GPE ['Afghanistan', 'Afghanistan ', 'FC Barcelona']\n",
            "CARDINAL ['4']\n",
            "NORP ['French ']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjzvTT4GQzCL"
      },
      "source": [
        ""
      ],
      "execution_count": 165,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfBhVVvLQl-a"
      },
      "source": [
        ""
      ],
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9owWxRFsRcD-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}